{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wadra/LLM_from_Scratch/blob/main/code/myedit/GPT4All-inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT4All CPU Interface\n",
        "\n",
        "In this notebook, we jump as quickly as we can into a command-line interaction with a GPT-4-like model that is on our own device."
      ],
      "metadata": {
        "id": "Powa-rm3ApS0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vVpLf3_fX3qK"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install nomic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gpt4all"
      ],
      "metadata": {
        "id": "CuSSxDNNCFpY",
        "outputId": "7701bd03-4f60-458c-9b8f-a37b7acb98d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpt4all\n",
            "  Downloading gpt4all-2.7.0-py3-none-manylinux1_x86_64.whl (29.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.8/29.8 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt4all) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt4all) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gpt4all) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2024.2.2)\n",
            "Installing collected packages: gpt4all\n",
            "Successfully installed gpt4all-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt4all import GPT4All"
      ],
      "metadata": {
        "id": "TjoampoyChOR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT4All(\"orca-mini-3b-gguf2-q4_0.gguf\")"
      ],
      "metadata": {
        "id": "XZ3Rz0grAneG",
        "outputId": "45a2bcae-b496-4516-dc9a-da02cd46b408",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|██████████| 1.98G/1.98G [01:13<00:00, 26.9MiB/s]\n",
            "Verifying: 100%|██████████| 1.98G/1.98G [00:04<00:00, 460MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate('What is a Large Language Model?')"
      ],
      "metadata": {
        "id": "H_Fscz0qBAAl"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "id": "VxyanGCZ_y2G",
        "outputId": "69b0ebaa-3d26-46a9-9120-ac1264c9783f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "A large language model (LLM) is a type of artificial intelligence that uses deep learning to generate human-like text. It is trained on vast amounts of data, typically in the form of books or websites, and can then produce new text that is similar in style and tone to the input it was trained on. LLMs are designed to be flexible and adaptable, allowing them to generate text in a wide range of styles and genres. They are often used for tasks such as language translation, summarization, and content generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt4all import GPT4All\n",
        "model = GPT4All(\"orca-mini-3b-gguf2-q4_0.gguf\")\n",
        "output = model.generate(\"The capital of France is \", max_tokens=3)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "UWUSosdeA3X6",
        "outputId": "99cc4674-e2fa-4e45-b942-f27ba69c0da5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 euros\n"
          ]
        }
      ]
    }
  ]
}